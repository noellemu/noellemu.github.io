---
title: 记一次缓存引起的数据异常 Bug 的排查过程
date: 2022-11-25 02:10:54
tags:
- Golang
- Cache
- Trouble Shooting
categories:
- Golang
- Trouble Shooting
---

## 背景

在我工作中开发的一个类似于 `Ansible` 和 `蓝鲸作业平台` 的作业平台中，由于 `MongoDB` 性能瓶颈导致 `NSQ` 消息队列积压严重，经讨论后决定将作业执行过程中的作业状态转换、作业日志存储等逻辑从直接读写 `MongoDB` 重构为基于 `sync.Map` 自行实现的进程内缓存——将作业数据以作业 ID 为 `key`，作业数据为 `value` 保存在 `sync.Map` 中（这个方案并不怎么好，后文会提到优化方案），并且启动了几个定时任务将 `sync.Map` 中的数据每隔 10 秒对执行中的作业执行历史数据进行落库操作。作业中所有步骤的执行历史信息在作业执行开始或跳过失败步骤时写入缓存和数据库，此后会进入一个类似 `Event Loop` 的无限 `for` 循环，每次循环都会根据作业执行目标的数量和状态判断作业的状态，若已成功结束（成功目标总数 `Succeed` = 执行目标总数 `Total`）则会立即发送下一个步骤给执行目标。

这样一来，业务逻辑就发生了变化：

- 优化前：作业执行消息到来 - 从 DB 中加载作业执行历史数据（主要是成功 / 失败 / 执行中三种状态的主机数量、重做次数等） - 根据执行历史数据和消息内容判断作业的状态 - 将状态和日志信息写入 DB。
- 优化后：作业执行消息到来 - 从缓存中加载作业执行历史数据（未命中缓存则查 DB）- 根据消息内容判断作业的状态 - 将状态和日志信息写入缓存 - 通过定时任务将作业执行历史信息落库（如果作业执行已经结束，则立即落库，降低资源消耗）。

显而易见的是，这种机制是 `Cache Aside`（先查缓存，缓存未命中则查数据库，修改时先写 DB 再写缓存）和 `Read Through / Write Behind`（写操作直接写缓存，然后由定时任务同步到 DB） 的结合体。

但在某一天，出现了一个诡异的恶性 Bug：当作业的第一个步骤执行失败时，对这个步骤执行跳过操作，之后的所有步骤都会不经执行而直接被设为成功。若查看作业执行历史，会发现步骤实际上执行了，日志也成功返回到服务端。此外，该 Bug 是随机发生的，在触发条件未知的情况下无法 100% 复现。

由于这个 Bug 与步骤和作业的状态判断逻辑有关，可能会引发严重的问题，于是立即展开排查与修复工作。

## 问题表现

- 该 Bug 是随机发生的，在不知晓原因的情况下无法 100% 复现。
- 必须有一个步骤（可以是第一步、第二步……第 n 步）失败并且对这个步骤执行跳过操作才有可能发生。
- 从作业执行开始到执行跳过操作的时间间隔非常短，最多 9 秒左右。
- 线上环境难以复现。

## 排查过程

首先拉取 Bug 发生时的日志，会发现被错误地设为成功的步骤的执行目标总数 `Total` 在缓存中被错误地设置为 0，且对应步骤在作业执行开始第一次写入缓存时 `Total` 是正确的，而在执行跳过操作以后第一次被写入缓存时 `Total` 就被设为 0，怀疑是缓存 Bug，但对所有写缓存的操作进行查看和打日志之后，未发现有某处修改了 `Total` 操作或未设置 `Total` 值。

然后尝试复现，在测试了超过 100 次之后，发现可以复现问题的时间似乎与缓存落库定时任务的执行时间有关——例如缓存定时任务每 10 秒运行一次，第一次运行的时间为 yy:yy:y3，那么作业开始执行的时间必须在 xx:xx:x3 到 xx:xx:(x-1)3 之间，且必须在 xx:xx:(x-1)3 之前对某一执行失败的步骤执行跳过操作，否则就无法复现，怀疑是某处对 `Total` 值写了缓存但没有写 DB。

查看步骤跳过的逻辑，发现 `Total` 值只写了缓存，没有写数据库，问题实锤。

排查和修复过程大约花费一天半，几乎全程独立完成。

## 根本原因

在作业执行开始，创建所有步骤的执行历史信息并写入缓存和数据库时，执行主机数量 `Total` 字段只写入了缓存，没有写入数据库，导致所有步骤在数据库中储存的 `Total` 字段被初始化为 `int` 类型的零值（0）。若第一个步骤在 10 秒内执行完毕并点击跳过，由于跳过操作与执行失败这两个事件之间可能相隔很久，为了防止缓存失效，在每次跳过操作时都会直接查询数据库。此时若缓存落库定时任务还没有执行，查询操作就会将这个错误的 `Total: 0` 查询出来并覆盖缓存中正确的值。由于此时 `Succeed`、`Failed`、`Total` 三个字段都等于 0，所以在 `for` 循环中虽然会将作业执行信息发送给执行目标，但执行状态在状态判断逻辑中会被直接判断为成功，进而导致了问题的产生。

当从作业开始执行的时间（设为 `tstart`）到对失败步骤执行跳过的时间（设为 `tend`）小于 `tstart` 之后缓存落库定时任务下一次执行的时间与 `tstart` 的间隔（设为 `ttask`），即 `tend - tstart < ttask` 时，缓存落库定时任务执行过一次之后，由于缓存中正确的 `Total` 值已经被写入数据库，查询操作读取到的就是正确的值，问题也就不会发生了。

而由于线上环境执行的作业由于执行目标比较多、脚本逻辑比较复杂等原因，单个步骤的运行时间一般都大于 10 秒，所以这个步骤在线上环境很难复现。

## 解决方案

1. 在作业执行开始时，将主机数量同时写入数据库和缓存，而不是只写缓存不写数据库。
2. 对于失败步骤的跳过操作，应直接查缓存，未命中缓存再查询数据库，而不是直接查询数据库。
3. 提高缓存落库定时任务的执行频率，降低问题发生的概率（当然这并不能从根本上解决问题）。

## 缓存功能的优化方案

1. 不使用 `sync.Map` 而是使用功能更加完备的进程内缓存框架（如 `gcache` 等）。
2. 可以只缓存与作业状态判断和日志更新相关的小部分数据（如作业 ID、作业的当前状态、重做次数）等，日志则写入消息队列并异步写入数据库（削峰填谷），降低内存占用。
3. 若考虑多点部署的问题，由于有进程内缓存意味着作业服务是有状态的，所以需要采用一致性哈希等机制将不同的作业 ID 映射到不同的作业服务实例上去，或者采用单独的缓存数据库实现跨进程的缓存（如 `Redis` 等）。
4. 日志不应该直接写到作业执行历史的某个数组类型的字段（如 `logs`）中，而应该写到专门的日志数据库（如 `ElasticSearch`）中，这不仅仅是因为这些数据库对海量数据的查询操作具有天然的优势，更可以将查询执行历史元数据和查询日志数据的操作分开，提升系统的性能（这一点考虑之后结合 `Metropolitan` 写篇文章讲讲）。
5. 现有的缓存机制在进程 `panic` 异常退出时会导致未落库的数据（执行中的作业执行历史数据）丢失，需要特别注意在 `recover` 兜底中将数据落库（怀疑之后数次日志丢失 / 作业状态判断不正确的偶现 bug 可能由此引起）。

## 附：缓存读写的业界推荐原则

- 增：先写 DB，再写缓存。
- 删：先删 DB，再删缓存（并发的情况下可能会读到脏数据，但概率比较小）。
- 改：先写 DB，再写缓存（同样可能有脏数据问题）。
- 查：先查缓存，未查到则查 DB，并把查到的数据同步到缓存。
